{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set_style('white')\n","plt.rcParams[\"patch.force_edgecolor\"] = True\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from plotly.offline import iplot, init_notebook_mode\n","import cufflinks as cf\n","import plotly.graph_objs as go\n","# import chart_studio.plotly as py\n","\n","init_notebook_mode(connected=True)\n","cf.go_offline(connected=True)\n","\n","# Set global theme\n","cf.set_config_file(world_readable=True, theme='pearl')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["df_original = pd.read_csv(\"/kaggle/input/hourly-weather-surface-brazil-southeast-region/sudeste.csv\", )\n"]},{"cell_type":"markdown","metadata":{},"source":["# Sample Data\n","Due to large data lets work with sampel data****"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df = df_original.sample(300000,random_state=101)"]},{"cell_type":"markdown","metadata":{},"source":["Dataset is too big so lets import in a chunk and work with the smallest one at first"]},{"cell_type":"markdown","metadata":{},"source":["## Info On Some Columns\n","\n","\n","1. Instant Air Temperature (celsius degrees) = temp\n","2. Maximum Air Temperature (celsius degrees) = tmin\n","3. Minimum Air Temperature (celsius degrees) = tmax\n","4. Relative Humidity of Air (%) =hmdy\n","5. Maximum Relative Air Humidity (%) =hmax\n","6. Minimum Relative Air Humidity (%) = hmin\n","7. Instant Dew Point (celsius degrees) = dewp\n","8. Maximum Dew Point (celsius degrees)=dmax\n","9. Minimum Dew Point Temperature (celsius degrees) = dmin\n","10. Instant Air Atmospheric Pressure (millibars) =stp\n","11. Maximum Air Atmospheric Pressure (millibars) = smax\n","12. Minimum Air Atmospheric Pressure (millibars)= smin\n","13. Instant Wind Speed (metres per second) = wdsp\n","14. Wind Direction (radius degrees) = wdct\n","15. Wind Gust Intensity (metres per second) = gust\n","16. Solar radiation  =  gbrd\n","17. Precipitation (milimetres) = prcp\n","18. Elevation = elvt\n","19. Observation Datetime = mdct\n","20. Observation Date = date\n","21. Station number (INMET number) for the location = inme\n","22. The year (2000-2016) : yr\n","23. The month (0-12) : mo\n","24. The day (0-31): da\n","25. The hour : hr\n","\n","*Not all the columns are mentioned in this list*"]},{"cell_type":"markdown","metadata":{},"source":["# Tidying Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","print(sample_df.iloc[:,:10].head(1))\n","print(\"==\"*20)\n","print(sample_df.iloc[:,10:20].head(1))\n","print(\"==\"*20)\n","\n","print(sample_df.iloc[:,20:].head(1))"]},{"cell_type":"markdown","metadata":{},"source":["Since there is no null values,under the assumption that all the data from various weather stations are valid and true, lets not filter the time of operation of weather stations and drop the wsids and inme, only city adn prov are enough."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df.drop(['wsnm','wsid','inme'],inplace=True, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Dealing with Time related columns"]},{"cell_type":"markdown","metadata":{},"source":["There are five columns on time of observatoin. They are mdct, date,yr, month and hour, which are same but separated into several sctions. Among them Date and Hour columns are useful and represent all in some form, so others can be dropped. The conversion of date column to datetime gave format error.So, first do padding of year and day column then, combie as data."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Padding of columns\n","# i.e  1->01, 2-> 02, so on.\n","sample_df.mo = sample_df.mo.astype(str).str.zfill(2)\n","sample_df.da = sample_df.da.astype(str).str.zfill(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df['Date'] =  sample_df[['yr','mo','da']].astype(str).agg(\"-\".join,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df['Date'] = pd.to_datetime(sample_df['Date'], format='%Y-%m-%d')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df.drop(['mdct','yr','da','hr','mo','date'], axis= 1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df.set_index(\"Date\", inplace=True,drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["Reset Index as Date column and the check for duplicates data on same date in same city and remove them"]},{"cell_type":"markdown","metadata":{},"source":["# Missing Values "]},{"cell_type":"markdown","metadata":{},"source":[" ## Missing Data Correlation\n"," \n"," Separate null columns and non-null ones, also the dataframes."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["null_cols = sample_df.columns[sample_df.isnull().sum()>0]\n","not_null_cols = sample_df.columns[sample_df.notnull().sum()==300000]\n","null_df= sample_df[null_cols]\n","not_null_df= sample_df[not_null_cols]\n"]},{"cell_type":"markdown","metadata":{},"source":["Creating a function that will plot the heatmap of any dataframe, plotting the correlation between the columns in the dataframes. Moreover, this customized heatmap will remove the repitition plotting only lower diagonal elements. Plot will also only dispaly  the columns correlaton that are >-0.25 and <0.25 which are generally considered high correlation."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def customized_heatmap(df):\n","    corr_df = df.corr()\n","#     print(corr_df)\n","    missing_df =corr_df.iloc[1:,:-1].copy()  \n","#     print(missing_df)\n","\n","    \n","    # Get only half portion of corr_df to avoid df, so create mask    \n","    mask = np.triu(np.ones_like(missing_df), k=1)\n","    \n","     \n","    # plot a heatmap of the values\n","    plt.figure(figsize=(20,14))\n","    ax = sns.heatmap(missing_df, vmin=-1, vmax=1, cbar=False,\n","                     cmap='coolwarm', mask=mask, annot=True)\n","    \n","    # format the text in the plot to make it easier to read\n","    for text in ax.texts:\n","        t = float(text.get_text())\n","        if -0.25 < t < 0.25:\n","            text.set_text('')\n","        else:\n","            text.set_text(round(t, 2))\n","        text.set_fontsize('x-large')\n","    plt.xticks( size='x-large')\n","    plt.yticks(rotation=0, size='x-large')\n","#     plt.savefig(\"Heatmap DF\")\n","    plt.show()\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Correlatoin of Null Columns in Heatmap (Fig 1.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["customized_heatmap(null_df)"]},{"cell_type":"markdown","metadata":{},"source":["The columns gust and wdsp have high correlation since they are both properties of wind. Moreover, both seem to be affected by properties of temperatures positively. \n","\n","Similary, Solar Radiation(GBRD) is also the affected by temperatures,gusts and windspeed respectively in order from higher to lower correlation. \n","\n","Dewpoint also has higher correlatin with temperature, while the highest with properties of humidity(hmin, hmax).\n","\n","Lets plot the properties that have higher correlation with each other in heatmap to visualize null rows."]},{"cell_type":"markdown","metadata":{},"source":["## Missing data percentage calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["null_counts = null_df.isnull().sum()\n","null_counts_pct = (null_counts / sample_df.shape[0])*100\n","\n","null_pct_df = pd.DataFrame({'null_counts': null_counts, 'null_pct': null_counts_pct})\n","\n","print(null_pct_df.T.astype(int))"]},{"cell_type":"markdown","metadata":{},"source":["For columns with smaller null count the percentage is very less to be shown in rounded values. "]},{"cell_type":"markdown","metadata":{},"source":["## Missing Data Visualization Using Heatmap "]},{"cell_type":"markdown","metadata":{},"source":["### Gust and Windspeed (fig 1.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Plot by sorting the values by gust\n","sorted_df_by_temp = sample_df[['gust','wdsp',\"temp\"]].sort_values(['temp'] )\n","sorted_df_by_gust = sample_df[['gust','wdsp',\"temp\"]].sort_values(['gust'] )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","sns.heatmap(sorted_df_by_gust.isnull(),cmap='coolwarm', cbar=False, yticklabels=False);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","sns.heatmap(sorted_df_by_temp.isnull(),cmap='coolwarm', cbar=False, yticklabels=False);"]},{"cell_type":"markdown","metadata":{},"source":["After sorting the columns it seems, for all the missing values in gust there are missing values in windspeed. But according to correlation heatmap, since gust and windspeed have higher realtion than with temperature.\n","Lets fill the rows  where windspeed has null values but gust does not with weighted mean of windspeed by gust column. i.e create a mask where wdsp is null and gust in not null. Then divide that range again into three interval just to be more precise.\n","\n","After that apply  interval technique to fill remaining rows with wighted mean of temperature columns but five intervals this time. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mask_gust_wdsp = sample_df['gust'].notnull() & sample_df['wdsp'].isnull()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create interval from mask\n","sample_df[mask_gust_wdsp].gust.value_counts(bins=3)"]},{"cell_type":"markdown","metadata":{},"source":["Create function that fills value by mask. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create three mean \n","first_range_mean_wdsp_by_gust= sample_df[sample_df['gust']<7.5]['wdsp'].mean()\n","second_range_mean_wdsp_by_gust=sample_df[((sample_df['gust']>=7.5) &(sample_df['gust']<15.0))]['wdsp'].mean()\n","third_range_mean_wdsp_by_gust=sample_df[sample_df['gust']>=15.0]['wdsp'].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#math is imported to check nan, np.nan wont work\n","def fill_wdsp_by_gust(col): \n","    \n","    \n","#Initialize relevant cols\n","    gust = col[0]\n","    wdsp = col[1]\n","    \n","    # If the value is nan\n","    #Assign by ranges declared above\n","    import math\n","    if (math.isnan(wdsp)):\n","        # Make sure gust in not nan\n","        if math.isnan(gust):\n","            pass\n","        elif (gust<7.5):\n","            return first_range_mean_wdsp_by_gust\n","        elif (gust>=7.5 ) and (gust<15.0):\n","            return second_range_mean_wdsp_by_gust\n","        elif (gust>=15.0):\n","            return third_range_mean_wdsp_by_gust\n","          #if not nan return as it is\n","    else:\n","        return wdsp\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df['wdsp'] = sample_df[['gust','wdsp']].apply(fill_wdsp_by_gust,axis=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["*Temparature* column seems to have high correlation with many columns. So, lets create a function that will fill the values of columns with different interval of times and their mean with in that intervals. for this project lets try 5 intervals of temp.\n","\n","Moreover, since temp is gonna determine the other columns lets drop few rows that are null in the temp column."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df.dropna(subset=['tmax','temp','tmin'],inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(sample_df['temp'].value_counts(bins=5).sort_index())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# this funciton will take two cols, temp and column to fill values in this order\n","# the second argument is the context\n","# From correlation table it seems temperature has high correlation with many columns so, context will clarify which value to fill \n","#by column name\n","#math is imported to check nan, np.nan wont work\n","\n","# Create five conditions\n","cond_1 = sample_df[sample_df['temp']<5.96]\n","cond_2 = sample_df[((sample_df['temp']>=5.96) & (sample_df['temp']<15.32))]\n","cond_3= sample_df[((sample_df['temp']>=15.32) & (sample_df['temp']<24.68))  ]\n","cond_4 = sample_df[((sample_df['temp']>=24.68) & (sample_df['temp']<34.04))  ]\n","cond_5 = sample_df[sample_df['temp']>=34.04]\n","\n","\n","# Create five ranges of mean according to above interval for windspeed\n","\n","first_range_mean_wdsp_by_temp=cond_1['wdsp'].mean()\n","second_range_mean_wdsp_by_temp= cond_2['wdsp'].mean()\n","third_range_mean_wdsp_by_temp= cond_3['wdsp'].mean()\n","fourth_range_mean_wdsp_by_temp= cond_4['wdsp'].mean()\n","fifth_range_mean_wdsp_by_temp= cond_5['wdsp'].mean()\n","\n","\n","# Create five ranges of mean according to above interval for gust\n","\n","first_range_mean_gust_by_temp= cond_1['gust'].mean()\n","second_range_mean_gust_by_temp=  cond_2['gust'].mean()\n","third_range_mean_gust_by_temp= cond_3['gust'].mean()\n","fourth_range_mean_gust_by_temp=  cond_4['gust'].mean()\n","fifth_range_mean_gust_by_temp=  cond_5['gust'].mean()\n","\n","\n","# Create five ranges of mean according to above interval for solar radiation\n","\n","first_range_mean_radiation_by_temp= cond_1['gbrd'].mean()\n","second_range_mean_radiation_by_temp=cond_2['gbrd'].mean()\n","third_range_mean_radiation_by_temp= cond_3['gbrd'].mean()\n","fourth_range_mean_radiation_by_temp=  cond_4['gbrd'].mean()\n","fifth_range_mean_radiation_by_temp=  cond_5['gbrd'].mean()\n","\n","\n","\n","def fill_missing_by_temp(col, context):\n","    import math\n","    #Initialize relevant cols\n","    temp = col[0]\n","    col_1_val = col[1]\n","    \n","    # Divide the task by context\n","    #Either for windspeed or for gust\n","    \n","    if context == \"wdsp\":\n","      \n","        # If the value is nan\n","        #Assign by ranges declared above\n","        if math.isnan(col_1_val):\n","            if(temp<5.96):\n","                return first_range_mean_wdsp_by_temp\n","            elif(temp>=5.96) and (temp<15.32):\n","                return second_range_mean_wdsp_by_temp\n","            elif(temp>=15.32) and (temp<24.68):\n","                return third_range_mean_wdsp_by_temp\n","            elif(temp>=24.68) and (temp<34.04):\n","                return fourth_range_mean_wdsp_by_temp\n","            elif(temp>=34.04):\n","                return fifth_range_mean_wdsp_by_temp\n","            #if not nan return as it is\n","        else:\n","            return col_1_val\n","        \n","    elif context==\"gbrd\":\n","         # If the value is nan\n","        #Assign by ranges declared above\n","        if math.isnan(col_1_val):\n","            if(temp<5.96):\n","                return first_range_mean_radiation_by_temp\n","            elif(temp>=5.96) and (temp<15.32):\n","                return second_range_mean_radiation_by_temp\n","            elif(temp>=15.32) and (temp<24.68):\n","                return third_range_mean_radiation_by_temp\n","            elif(temp>=24.68) and (temp<34.04):\n","                return fourth_range_mean_radiation_by_temp\n","            elif(temp>=34.04):\n","                return fifth_range_mean_radiation_by_temp\n","            #if not nan return as it is\n","        else:\n","            return col_1_val\n","        \n","    else:\n","         # If the value is nan\n","        #Assign by ranges declared above\n","        if math.isnan(col_1_val):\n","            if(temp<5.96):\n","                return first_range_mean_gust_by_temp\n","            elif(temp>=5.96) and (temp<15.32):\n","                return second_range_mean_gust_by_temp\n","            elif(temp>=15.32) and (temp<24.68):\n","                return third_range_mean_gust_by_temp\n","            elif(temp>=24.68) and (temp<34.04):\n","                return fourth_range_mean_gust_by_temp\n","            elif(temp>=34.04):\n","                return fifth_range_mean_gust_by_temp\n","            #if not nan return as it is\n","        else:\n","            return col_1_val\n","    \n","        \n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df['wdsp'] = sample_df[['temp','wdsp']].apply(fill_missing_by_temp,context =\"wdsp\",axis=1)\n","sample_df['gust'] = sample_df[['temp','gust']].apply(fill_missing_by_temp,context= \"gust\", axis=1)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Solar Radiation\n","\n","\n","Solar radiation has high correaltion with temp so, lets checkout those columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Plot by sorting the values by temp\n","gbrd_df_by_temp = sample_df[['temp',\"gbrd\" ]].sort_values(['temp'] )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","sns.heatmap(gbrd_df_by_temp[['temp',\"gbrd\"]].isnull(),cmap='coolwarm', cbar=False, yticklabels=False);\n"]},{"cell_type":"markdown","metadata":{},"source":["The radiation column has very high missing values and quite scattered. Lets apply temp fill function with context as gbrd."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df['gbrd'] = sample_df[['temp','gbrd']].apply(fill_missing_by_temp,context= \"gbrd\", axis=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["Just for the sake of argument fill the missing values in dewp, dmax with average values"]},{"cell_type":"markdown","metadata":{},"source":["### Dewpoint \n","\n","\n","Dew point has highest correlation with humyidity and then with temperature.Lets fill their values by dewp"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# First drop hmin and hmax na values\n","\n","sample_df.dropna(subset=['hmax','hmin'],inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["\n","Plotting heatmap will not be fruitfull because the relative size of null values to the total lenght of df is quite low so, the heatmap will not show the null values. \n","Lets just use pct table we created earlier.\n","\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(null_pct_df.T.astype(int))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Check out intervals\n","\n","sample_df.hmdy.value_counts(bins=3).sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create three mean \n","first_range_mean_dwep_by_humidity= sample_df[sample_df['hmdy']<33.333]['dewp'].mean()\n","second_range_mean_dwep_by_humidity=sample_df[((sample_df['hmdy']>=33.333) &(sample_df['hmdy']<66.667))]['dewp'].mean()\n","third_range_mean_dwep_by_humidity=sample_df[sample_df['hmdy']>= 66.667 ]['dewp'].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#math is imported to check nan, np.nan wont work\n","def fill_dewp_by_humidity(col): \n","    \n","    \n","#Initialize relevant cols\n","    hmdy = col[0]\n","    dewp = col[1]\n","    \n","    # If the value is nan\n","    #Assign by ranges declared above\n","    import math\n","    if math.isnan(dewp):\n","        if (hmdy<33.333):\n","            return first_range_mean_dwep_by_humidity\n","        elif (hmdy>=33.333 ) and (hmdy<66.667):\n","            return second_range_mean_dwep_by_humidity\n","        elif (hmdy>=66.667):\n","            return third_range_mean_dwep_by_humidity\n","          #if not nan return as it is\n","    else:\n","        return dewp\n","    "]},{"cell_type":"markdown","metadata":{},"source":["Lets apply the func to all dewp variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_df['dewp'] = sample_df[[\"hmdy\", \"dewp\"]].apply(fill_dewp_by_humidity, axis=1)\n","sample_df['dmin'] = sample_df[[\"hmdy\", \"dmin\"]].apply(fill_dewp_by_humidity, axis=1)\n","sample_df['dmax'] = sample_df[[\"hmdy\", \"dmax\"]].apply(fill_dewp_by_humidity, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Only column with null value is prcp, which is also a dependent column. Hence leave it at that for now. Create new df for analysis without prcp. Moreover, some analysis is faster using the normalized version so lets created a normalized dataframe too.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# New df without prcp\n","df = sample_df[['elvt', 'lat', 'lon','city', 'prov', 'stp', 'smax', 'smin', 'gbrd', 'temp',\n","       'dewp', 'tmax', 'dmax', 'tmin', 'dmin', 'hmdy', 'hmax', 'hmin', 'wdsp',\n","       'wdct', 'gust']].copy()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","\n","#Make a copy from the original df first\n","\n","normed_df = df.copy()\n","# EXtract cols with non-string values\n","float_cols = df.columns[df.dtypes ==\"float64\" ].tolist()\n","# Normed df \n","normed_df[float_cols] =scaler.fit_transform(normed_df[float_cols])"]},{"cell_type":"markdown","metadata":{},"source":["### Data share By Province"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['prov'].value_counts(normalize=True).plot.pie(figsize=(8,10),autopct = '%.1f%%',\n","                                                 labels=['Minas Gerais','São Paulo','Rio de Janeiro','Espírito Santo'])\n","plt.xlabel(\"\")\n","plt.ylabel(\"\")\n","plt.title('Weather Data by Province');\n","# plt.savefig(\"Provinces Proportions\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["It seems abot 50% of the data is related to *Minas Gerlas* province of Brazil"]},{"cell_type":"markdown","metadata":{},"source":["#### Columns and Measurement scale\n","\n","City and Province are categorical, nominal values. All other columns are quantitative,continuous and ratio scale. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gr_by_city = normed_df[['prov','city', 'stp', 'smax', 'smin', 'gbrd', 'temp',\n","                  'dewp', 'tmax', 'dmax', 'tmin', 'dmin', 'hmdy', \n","                  'hmax', 'hmin', 'wdsp','wdct', 'gust']].groupby('city').mean()\n","\n","gr_by_prov = normed_df[['prov','city', 'stp', 'smax', 'smin', 'gbrd', 'temp',\n","                             'dewp', 'tmax', 'dmax', 'tmin', 'dmin','hmdy', \n","                        'hmax', 'hmin', 'wdsp','wdct', 'gust']].groupby(['prov']).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gr_by_city.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gr_by_yr_prov = normed_df.groupby([normed_df.index.year,'prov']).mean().unstack(level=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["layout = dict(xaxis_title=\"City\",\n","              yaxis_title=\"Frequency Normalized\",\n","              title=\"Avg. Temp by City\")\n","gr_by_city[('temp')].iplot(kind=\"bar\", layout=layout);"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Change name for better labels in legend\n","# 'stp', 'smax', 'smin', 'gbrd', 'temp', 'dewp', 'tmax', 'dmax', 'tmin',\n","#        'dmin', 'hmdy', 'hmax', 'hmin', 'wdsp', 'wdct', 'gust'\n","cols_to_plot =['stp','gbrd', 'temp', 'dewp', 'hmdy', 'wdsp', 'gust']\n","cols_changed_name ={\"Air Pressure\",\"Solar Radiation\", \"Temperature\",\"Dew Point\",\"Humidity\", \"Windspeed\",\"Gust\"}\n","\n","\n","temp_df = gr_by_prov[cols_to_plot]\n","\n","temp_df.columns = cols_changed_name\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["layout = dict(xaxis = dict( tickvals =[\"ES\",\"MG\",\"RJ\",\"SP\"],\n","                           ticktext=['Espírito Santo','Minas Gerais','Rio de Janeiro','São Paulo']),\n","             legend_title_text='Weather Factors',\n","              xaxis_title=\"Provinces\",\n","              yaxis_title=\"Frequency Normalized\",\n","              title = \"Avg Weather Factors by Provinces\",)\n","temp_df.iplot(kind=\"bar\",  layout=layout);\n","#names = [\"Air pressure\",\"Solar Radiation\", \"Temperature\", \"Dewpoint\", \"Humidity\",\"Windspeed\",\"Gust\"])\n","\n","          "]},{"cell_type":"markdown","metadata":{},"source":["Province *Espirito Santo* is taking the top spot on average weather factors by province. While *Minas Gerias* and *Rio de Janerio* both share similar triats. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sample_df[['prcp',\"city\"]].groupby(['city', df.index.year]).mean().unstack(level=1)\n","#\n","temp_yearly=pd.pivot_table(data= sample_df,index=\"city\", columns=sample_df.index.year,values=\"temp\")\n","# temp_yearly_normed=pd.pivot_table(data= normed_df,index=\"city\", columns=sample_df.index.year,values=\"temp\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["l= dict(showlegend=False, title=\"Yearly Average Temperature\", xaxis_title=\"Year\", yaxis_title=\"Frequecny\")\n","temp_yearly.iplot(kind=\"box\",layout=l)"]},{"cell_type":"markdown","metadata":{},"source":["Year 2011 has least fluctuations, data in early 2000's are incomplete data so can't be compared properly. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# temp_yearly.iplot(secondary_y = min_values)\n","layout= dict(yaxis_title=\"Frequency\",xaxis_title=\"City\", title=\"Average Yearly Temperature by Cities\",)\n","temp_yearly.iplot(kind=\"scatter\", mode=\"lines+markers\", layout=layout)"]},{"cell_type":"markdown","metadata":{},"source":["In 2016, which is the latest record, Guaruja,is the coldest city and \"Sao Goncalo\" is the hottest city. "]},{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":["Looks like 2013-2016 have been periods of ups and downs. Average temperature  fell from about 20.5 to  16.5 from 2013 t0 2014 and again rose to about 22 degrees on average and theres slopy journey again to 2016."]},{"cell_type":"markdown","metadata":{},"source":["Lets normalize the dataset first to neutralize different units"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["scaler = MinMaxScaler()\n","\n","# Take a copy of sample_df\n","sample_df_norm = sample_df.copy()\n","\n","#Columns to normalize\n","cols_to_norm = ['elvt', 'lat', 'lon', 'prcp', 'stp', 'smax', 'smin','gbrd', 'temp', 'dewp', 'tmax', 'dmax', 'tmin', 'dmin',\n","                'hmdy', 'hmax','hmin', 'wdsp', 'wdct', 'gust']\n","\n","#Normalize numeric scales\n","sample_df_norm.loc[:,cols_to_norm] = scaler.fit_transform(sample_df.loc[:,cols_to_norm])"]},{"cell_type":"markdown","metadata":{},"source":["## Temperature\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["### Seasonal Fluctuations- Tempearature\n","\n","# Assuming 4 mnths seasons\n","# Lets obsorve seasoal fluctuation\n","layout= dict(yaxis_title=\"Frequency\",xaxis_title=\"Years\", title=\"Average Seasonal Temperature Distribution\",)\n","sample_df_norm[['temp','tmin',\"tmax\"]].resample('M').mean().rolling(4).mean().iplot(kind=\"bar\",layout=layout )\n"]},{"cell_type":"markdown","metadata":{},"source":["On average fluctation was noticeable during yr 2001 and 2006. Slightly however, there is constant sloppy decrease in temp from 2004 to 2006, lowest temperatures comparing to other years. \n","\n","From year 2010-2016 the temperatures are steady and warmer."]},{"cell_type":"markdown","metadata":{},"source":["### Temperature dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["temperature_df=sample_df[['city','temp','tmax','tmin']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_temp_by_choice(yr,mo,c):\n","    # Get temperature record by given month and year\n","    temp_m_y_c=temperature_df[(temperature_df.index.year==yr) & (temperature_df.index.month==mo) & (temperature_df.city == c)]\n","    \n","    \n","    # Extract given city from  above record\n","    temp_max = temp_m_y_c.max()\n","    temp_min = temp_m_y_c.min()\n","    print(temp_max)\n","    print(temp_min)\n","    print(\"==\"*20)\n","#     return temp_max, temp_min"]},{"cell_type":"markdown","metadata":{},"source":["Now we can extract temp records of our choice lets, try few."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["get_temp_by_choice(2008,10,\"Viçosa\")\n","get_temp_by_choice(2008,10,\"Valença\")\n","get_temp_by_choice(2008,10,\"Montalvânia\")\n","get_temp_by_choice(2008,10,\"Itaobim\")"]},{"cell_type":"markdown","metadata":{},"source":["In October of 2008, *Viçosa* was the coldest and *Montalvânia* was the hottest one."]},{"cell_type":"markdown","metadata":{},"source":["### Plot By Custom Choice\n","The following function will plot Average temperature of given list of citeis in  bar  plot some timeline of choice."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["timeline={\"Weekly\":\"W\",\"Monthly\":\"M\",\"Yearly\":\"Y\"}\n","\n","\n","def temp_analysis(cities, time_line=\"Yearly\"):\n","    \"\"\"provide timeline among weekly, monthly or yearly. The default value is Yearly.\n","     Function takes  two  arguments \n","     1. cities, which is list of cites\n","     2. time_line, which has three values, weekly, yearly, or monthly\n","    for eg: temp_analysis(['Viçosa','Mantena','Formiga',\"São João del Rei\",\"Juiz de Fora\" ], \"Yearly\")\n","\n","\"\"\"\n","    print(temp_analysis.__doc__)\n","    \n","    c =pd.DataFrame()\n","    # Extract given city from  above record\n","    for city in cities:\n","        c= c.append(temperature_df[temperature_df['city'] == city])\n","    \n","    # Resample it on average\n","    #title() takes care of uppercase and lowercase confusion\n","    d=c.resample(timeline[time_line.title()]).mean()\n","    print(\"Temperature of cities {}\".format(cities))\n","\n","   \n","    # Plot the data\n","    layout= dict( title='Avg Temperatures {}'.format(time_line.upper()), xaxis_title=\"Years\", yaxis_title=\"Temperatures\")\n","    d[['tmax','tmin','temp']].iplot(kind=\"bar\", layout=layout)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["temp_analysis(['Viçosa','Mantena','Formiga',\"São João del Rei\",\"Juiz de Fora\" ], \"Weekly\")"]},{"cell_type":"markdown","metadata":{},"source":["If the weekly timeline is provided on zooming in the max temp week is From Jan 18-25,2015"]},{"cell_type":"markdown","metadata":{},"source":["## Airpressure"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming 4 mnths seasons\n","# Lets obsorve seasoal fluctuation\n","layout= dict(yaxis_title=\"Frequency\",xaxis_title=\"Years\", title=\"Average Seasonal Pressure Distribution\",)\n","sample_df_norm[['stp','smin',\"smax\"]].resample('M').mean().rolling(4).mean().iplot(kind=\"bar\",layout=layout )\n"]},{"cell_type":"markdown","metadata":{},"source":["Air pressure and temperature  seasonal dist show similar behaviour accoring to plot."]},{"cell_type":"markdown","metadata":{},"source":["### Sketching the chart for specific air pressure values with year, month and city\n","\n","Create a function which shows the data from given year, month and city name then plots it"]},{"cell_type":"markdown","metadata":{},"source":["### Airpressure Dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# df for airpressure only\n","air_df=sample_df[['city','prov','stp','smax','smin']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def monthly_plot(m,y,cities):\n","    '''  Function takes  three arguments month in number, year in int and list of cities to compare\n","    for eg: monthly_plot(10,2008,['Viçosa','Mantena','Formiga',\"São João del Rei\",\"Juiz de Fora\" ])\n","\n","\n","    '''\n","    print(monthly_plot.__doc__)\n","  \n","    # Get pressure record by given month and year\n","    air_m_y=air_df[(air_df.index.month == m) & (air_df.index.year == y)]\n","    air_m_y_c =pd.DataFrame()\n","    # Extract given city from  above record\n","    for c in cities:\n","        air_m_y_c = air_m_y_c.append(air_m_y[air_m_y['city'] == c])\n","    \n","    grp_by_c = air_m_y_c.groupby('city').mean()\n","    layout= dict( title='Avg Air pressure of city {}-{}'.format(m,y), xaxis_title=\"Cities\", yaxis_title=\"Air pressure\")\n","    grp_by_c.iplot(kind=\"bar\",layout=layout)"]},{"cell_type":"markdown","metadata":{},"source":["Lets check some cities from same province first, during same times.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# For prov \"MG\"\n","minas_cities = air_df[air_df.prov==\"MG\"].city\n","# lets just use 5 cities at random\n","minas_cities[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["monthly_plot(10,2008,['Viçosa','Mantena','Formiga',\"São João del Rei\",\"Juiz de Fora\" ])"]},{"cell_type":"markdown","metadata":{},"source":["City *Maneta* is the only one with air pressure in the upper range around the year, while *Vicosa's* in the middle range and other 3 are in lower range."]},{"cell_type":"markdown","metadata":{},"source":["## Gusty City of All Time"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gust_yr_city = sample_df_norm.pivot_table(columns=sample_df_norm.index.year,index='city',values=['gust'],aggfunc='mean').stack()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gusty_city= gust_yr_city.loc[gust_yr_city['gust']==gust_yr_city.gust.max()].index[0]\n","gusty_city"]},{"cell_type":"markdown","metadata":{},"source":["Among the cities, the gusty city is \"Petrópolis\" in year 2007."]},{"cell_type":"markdown","metadata":{},"source":["# Variables and Correlations"]},{"cell_type":"markdown","metadata":{},"source":["## Heatmap of Corr"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["customized_heatmap(normed_df)"]},{"cell_type":"markdown","metadata":{},"source":["Above heatmap shows obvious high correlation between similar variables like (temp,tmax and tmin), (stp,smax and smin) and so on. However, it shows higher correaltion btween variables of dew points with humidity. It can implied that more due forms for humidity. The jointplot \"Dew and Humidity\" also proves the point. Moreover, the aspect of temperature(temp,tmax and tmin) and airpressure (stp,smax and smin) also have high correlation between them.  \n","\n","It appears aspects wind-speeds have very low correlatoin with humidity. Windspeed and Humidity plot below will make this more clear.  "]},{"cell_type":"markdown","metadata":{},"source":["### Dew and Humidity"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# fig = plt.figure(figsize=(10,10))\n","sns.jointplot(normed_df['hmdy'],normed_df['dewp']);\n","# plt.savefig(\"Humidity vs Dewpoint\")"]},{"cell_type":"markdown","metadata":{},"source":["### Humidity and WindSpeed"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize=(10,10))\n","sns.jointplot(normed_df['wdsp'],normed_df['hmdy'], kind='kde');\n","plt.savefig(\"Windspped vs Humidty Density Plot\")"]},{"cell_type":"markdown","metadata":{},"source":["The plot is denser on the left and botteom. So, it seems wind-speed and humidity are reversely proportional to each other. Increase in wind-speed causes decrease in humidity. Also both appear right skewed."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
